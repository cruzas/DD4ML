#!/bin/bash
#SBATCH --job-name=${job_name}    # job_name defined in submit_jobs.sh script
#SBATCH --output=./log_files/${job_name}.out
#SBATCH --error=./log_files/${job_name}.err
#SBATCH --nodes=1
#SBATCH --time=${time}
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-core=1

# NOTE: these two should match to assign one task per GPU
#SBATCH --ntasks-per-node=${ntasks_per_node}   # Dynamically set tasks per node
#SBATCH --gres=gpu:${ntasks_per_node}           # Dynamically set tasks per node

#SBATCH --hint=nomultithread
#SBATCH --constraint=gpu
#SBATCH --partition=${partition} # Dynamically set partition
#SBATCH --exclusive
source ~/.slurm_env # You should define SLURM_ACCOUNT in this file
#SBATCH --account=$SLURM_ACCOUNT

# Source common environment files
source ~/.tests_runpath # You should define TESTS_RUNPATH in this file
cd "$TESTS_RUNPATH" || exit

# Load required modules
module load cray/23.12
module load cray-python/3.11.5

# Set environment variables
if [ -z "$SLURM_CPUS_PER_TASK" ]; then
    export OMP_NUM_THREADS=1
else
    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
fi
export NCCL_DEBUG=ERROR   # Set to INFO for debugging
export NCCL_P2P_DISABLE=0 # Enable peer-to-peer communication
export NCCL_SHM_DISABLE=0 # Enable shared memory communication

# Optionally source WandB environment
if [ "${use_wandb}" = "1" ]; then
    source ~/.wandb_env
    export WANDB_MODE=online
fi

extra_flags=""
if [[ "$optimizer" == "apts_ip" ]]; then
    extra_flags="--use_pmw" # only necessary for inexact gradient version of APTS
fi

# Execute the Python script
echo "Test started $(date)"
export CUDA_VISIBLE_DEVICES=0,1,2,3
srun --kill-on-bad-exit /opt/cray/pe/python/3.11.5/bin/python -u "${script}" \
    $extra_flags \
    --optimizer="${optimizer}" \
    --sweep_config="${config_file}" \
    --num_stages=${num_stages} \
    --num_subdomains=${num_subd} \
    --num_replicas_per_subdomain=${num_rep} \
    --project=${PROJECT} \
    --trial_num=${trial} \
    --use_seed

echo "Test done $(date)"
rm core_nid*
