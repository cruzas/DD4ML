#!/bin/bash
#SBATCH --job-name=${JOB_NAME} # JOB_NAME should be defined in [submit_job].sh script
#SBATCH --output=./log_files/${JOB_NAME}.out
#SBATCH --error=./log_files/${JOB_NAME}.err
#SBATCH --nodes=1
#SBATCH --time=00:30:00
#SBATCH --cpus-per-task=1
#SBATCH --ntasks-per-core=1

# NOTE: these two should match to assign one task per GPU
#SBATCH --ntasks-per-node=${NTASKS_PER_NODE}   # Dynamically set tasks per node
#SBATCH --gres=gpu:${NTASKS_PER_NODE}   # Dynamically set tasks per node

#SBATCH --hint=nomultithread
#SBATCH --constraint=gpu
#SBATCH --partition=debug
#SBATCH --exclusive
source ~/.slurm_env # You should define SLURM_ACCOUNT in this file
#SBATCH --account=$SLURM_ACCOUNT

# Source common environment files
source ~/.tests_runpath # You should define TESTS_RUNPATH in this file
cd "$TESTS_RUNPATH" || exit

# Load required modules
module load cray/23.12
module load cray-python/3.11.5

# Set environment variables
if [ -z "$SLURM_CPUS_PER_TASK" ]; then
    export OMP_NUM_THREADS=1
else
    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
fi
export NCCL_DEBUG=ERROR   # Set to INFO for debugging
export NCCL_P2P_DISABLE=0 # Enable peer-to-peer communication
export NCCL_SHM_DISABLE=0 # Enable shared memory communication

# Optionally source WandB environment
if [ "${USE_WANDB}" = "1" ]; then
    source ~/.wandb_env
    export WANDB_MODE=online
fi

# Execute the Python script
echo "Test started $(date)"
export CUDA_VISIBLE_DEVICES=0,1,2,3
srun --kill-on-bad-exit /opt/cray/pe/python/3.11.5/bin/python -u "${SCRIPT}" --sweep_config="${CONFIG_FILE}" --use_pmw --num_stages=${NUM_STAGES} --num_subdomains=${NUM_SUBD} --num_replicas_per_subdomain=${NUM_REP} --project=${OPTIMIZER}_tests --trials=3
echo "Test done $(date)"
rm core_nid*
